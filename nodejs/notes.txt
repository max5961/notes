-------------------------------------------------------------------------------
NODE.JS
-------------------------------------------------------------------------------
Node.js is an open-source, cross-platform JavaScript runtime environment.


WHAT IS ECMASCRIPT?
-------------------------------------------------------------------------------
In 1993, the first web browser with a user interface was released.  This web
browser was called Mosaic.  In 1994, the lead developers of Mosaic founded a
company called Netscape and released a more polished browser called Netscape
Navigator.  Although this was a popular browser, it lacked the capability for
dynamic behavior.  There was no interactivity after a page was loaded (web pages
were static only).  In order to make webpages interactive, Netscape created a
new scripting language called JavaScript in 1995.  The name JavaScript was
purely for marketing purposes as Java was a popular language at the time.

Around the same time in 1995, Microsoft debuted Internet Explorer and realized
that JavaScript fundamentally changed the user experience of the web and wanted
the same for Internet Explorer.  In 1996, Microsoft reverse-engineered the
Netscape Navigator interpreter to create its own scripting language called
JScript.

Because Netscape Navigator used JavaScript and Internet Explorer used JScript,
it made it difficult for developers to make their websites work well in both
browsers.  The differences were so significant that websites often had badges
for "Best viewed in Netscape" or "Best viewed in Internet Explorer".

In 1996, Netscape submitted JavaScript to Ecma International.  Ecma
International is an industry association dedicated to the standardization of
information and communication systems.  Netscape wanted a standard specification
that all browser vendors could conform to as it would help keep other
implementations consistent across browsers.

For each new specification, Ecma provides a standard specification and a
committee.  In the the case of JavaScript, the standard is called ECMA-262 and
the committe that works on ECMA-262 is called Technical Committee 29 (TC39).
Ecma decided to use the term "ECMAScript" to talk about the official language.
The reason for this is because Oracle (Microsystems) owns the trademark
"JavaScript".  ECMAScript refers to the standard language whereas JavaScript is
what we use in practice and builds on top of ECMAScript.

The versions of ECMAScript are:

    ECMAScript 1 (1997)
    ECMAScript 2 (1998)
    ECMAScript 3 (1999)
    ECMAScript 4 (NEVER RELEASED)
    ECMAScript 5 (2009)
    ECMAScript 2015 or ES6 (2015)

ECMA-262 is the language specification
ECMAScript is the language that implements ECMA-262
JavaScript is basically ECMAScript at its core, but builds on top of that.


CHROME'S V8 ENGINE
-------------------------------------------------------------------------------
https://github.com/v8/v8/tree/main
-------------------------------------------------------------------------------
https://v8.dev/docs
-------------------------------------------------------------------------------
A JavaScipt engine is a program that converts JavaScript code into binary that
allows a computer to perform specific tasks.  It would be incorrect to say that
JavaScript is compiled to C++, however, Chrome's V8 JavaScript engine is written
in C++ and therefore the V8 engine leverages C++ to parse and compile the
JavaScript into binary.  This process involves several major steps:

    1. Parse the JS code to create an Abstract Syntax Tree (AST)
    2. Generate bytecode from the AST
    3. Just In Time compile the bytecode into binary

If JavaScript compiles into machine code, then why is it significantly slower in
comparison to C/C++?

    - modern JavaScript engines like Chrome's V8 engine use JUST IN TIME (JIT)
      compilation which involves compilation at runtime.  C/C++ are
      compiled to machine code ahead of time, which removes the overhead of
      compiling at runtime.  Given that C/C++ are compiled ahead of time,
      compilers can spend more time optimizing code during the compilation
      process.  The resulting binary code is more optimized.

    - JavaScript is a dynamically typed language which means that the types are
      determined at runtime.  This flexibility comes at a cost because the
      compiler must constantly make type checks and inferences, adding overhead
      to the execution.  C/C++ are statically typed, which means that the
      types are known at compile time.  This allows for more efficient
      compilation.

    - JavaScript's use of automatic memory adds overhead

    - JavaScript's execution is typically confined to a browser or a Node.js
      environment, which is not as efficient as interacting directly on the
      operating system as is the case with C/C++.

JavaScript engines are typically developed by web browser vendors, and every
major browser has their own.  Here are some of the major JavaScript engines.

    V8 - Open-source JavaScript Engine developed by Google for Chrome

    SpiderMonkey - JavaScript Engine powering Mozilla Firefox

    JavaScriptCore - Open-source JavaScript Engine developed by Apple for Safari

    Chakra - JavaScript Engine for the original Microsoft Edge (The latest
    version of Edge uses V8)

Node.js is built on Chrome's V8 engine by Google.  V8 implements ECMAScript as
specified in ECMA-262.  Because V8 is written in C++, V8 can be embedded into
any C++ application.  The ability for V8 to be embedded into C++ applications is
what led to the development of Node.js and allows you to add new features to
JavaScript itself.  While V8 can be embedded into any C++ application, it is not
as seamless for Node.js.

Since C++ is great for lower level operations like file handling, database
connections, and network operations, Node.js was able to add all of those
features as well.


JAVASCRIPT RUNTIME
-------------------------------------------------------------------------------
A JavaScript Runtime is an environment which provides all the necessary
components in order to use and run a JavaScript program.  While every browser
has a JavaScript Engine, the JavaScript Engine is only one component in the
JavaScript Runtime.  The JavaScript Runtime refers to where JavaScript code is
compiled as well as executed, which is differs from what 'runtime' means in many
programming languages where runtime and compile time are separate.

Using the Chrome Browser as an example, the below demonstrates the JavaScript
Runtime:
                                JavaScript code
                                      |
  |----------JAVASCIRPT RUNTIME-------V-----------------|
  |                                                     |
  |                                                     |
  |   V8 JavaScript Engine --> Web/Browser APIs ---|    |
  |           ↑                                    |    |
  |           |                                    |    |
  |       Event Loop                               |    |
  |           ↑                                    |    |
  |           |                                    |    |
  |           |--------------MicroTask Queue <-----|    |
  |           |---MacroTask / Callback Queue <-----|    |
  |                                                     |
  |                                                     |
  |------------------------------------------------------


    - The V8 JavaScript engine consists of
        - the call stack (where code is executed)
        - the heap where memory is stored.  While the heap refers to dynamic
          memory allocation in C++, it is correct to call the location where
          memory is stored the heap in the JavaScript V8 engine despite
          JavaScript automatically managing all memory.

    - Web / Browser APIs add extra functionality to the JavaScript Engine, but
      these extra functionalities are NOT part of the JavaScript language
      itself.  All of the API's are provided by the Engine
        - Document Object Model (DOM) (browser only)
        - Timers such as setTimeout and setInterval
        - Promises
        - Local Storage (browser only)
        - and more...

    - The MicroTask Queue and MacroTask (callback) Queue is where asynchronous
      tasks wait until they can be executed.

    - The Event Loop is what orchestrates the the execution of tasks in the
      Micro and Macro Task Queues.  It constantly checks to see if the call
      stack is empty.  If it is empty it first checks the MICRO task queue, and
      if it is not empty, it pops off the next task to be executed.  If the
      MICRO task queue is empty, it checks the MACRO task queue and pops off the
      next task if it is not empty.  These tasks which are popped off are
      executed on the call stack.  For non JS operations, they are triggered by
      some JS command which will still be executed on the call stack, but these
      non-JS operations such as DOM manipulations are handled outside of the
      call stack.


WHAT IS NODE.JS
-------------------------------------------------------------------------------
Node.js is an open-source, cross-platform JavaScript runtime environment.
Node.js provides all the necessary components to be able to run a JavaScript
program OUTSIDE OF THE BROWSER.

Node.js was introduced in 2009 and up until that point, JavaScript could only be
run inside a browser.

Node.js has the capability to build:

    - traditional websites
    - backend services like APIs
    - real-time applications
    - streaming services
    - CLI tools
    - multiplayer games

Node.js Runtime does not have access to Web APIs whereas Browser JavaScript
Runtimes do.  However, Browser Runtimes don't have access to the APIs that only
Node has access to.

The next section on the Node.js source code goes into more detail about the core
components of Node.js, but a brief overview is as follows:

    Node.js libraries provide a JS interface to run the underlying C++ source
    code which makes heavy use of Libuv to perform many of its operations.

Node.js is not a language, nor a framework.  It is a JavaScript Runtime
Environment that is capable of executing JS outside of the browser.  The Node.js
runtime allows to execute not only the standard ECMAScript language, but also
features which are made available through C++ bindings using the V8 engine.  The
Node.js Standard Library is collectively referred to as the Node.js API.
Similar to how the DOM API exists for Browsers, the Node.js API exists for
Node (non-browser JS Runtime)

NODE.JS SOURCE CODE (https://github.com/nodejs/node)
-------------------------------------------------------------------------------

Dependencies Folder (https://github.com/nodejs/node/tree/main/deps)
    - This lists all of the dependencies required by Node.js.  Two very
      important dependences are: V8, uv
    - uv is an open-source library called libuv which provides Node.js
      event-driven, non-blocking I/O capabalities making it suitable for file
      system and networking.

Source Folder (https://github.com/nodejs/node/tree/main/src)
    - This folder contains the C++ source code of the Node.js Runtime.  This
      code interacts with V8 for JavaScript execution and libuv for event loop,
      I/O operations, among other functionalities.

Lib Folder (https://github.com/nodejs/node/tree/main/lib)
    - The lib folder contains JavaScript code which forms the standard library
      of Node.js.
    - This code provides an interface to the underlying C++ functionalities,
      making them accessible without needing to write C++ code. When you use
      require/import to load modules they are typically coming from this folder
      (locally installed npm modules are also imported using require/import but
      from the local node_modules folder)


EXECUTING JAVASCRIPT WITH NODE
-------------------------------------------------------------------------------
You can execute JavaScript code by entering the node command in the terminal
which starts an interactive shell which processes Node.js expressions.  This
does the process: Read, Evaluate, Print, Loop (REPL).  console.log("hello
world"); in the Node shell prints Hello World followed by undefined because it
always prints out the evaluation of an expression.  Likewise, you could enter: 2
+ 2 and it would print out 4 even though it was never explicitly passed to a
console.log() statement.  That is because it reads, evaluates, and then prints
the evaluation.  It then continues to loop with a prompt until the user signals
to to quit Node (ctrl + c, or .exit).

The preferred (or more practical) way of executing node is by running the node
command followed by the path/to/file.  You can also omit the .js file extension
if you wanted to.


BROWSER VS NODEJS
-------------------------------------------------------------------------------
In the browser, most of the time you are interacting with the DOM, or other
Web Platform APIs like cookies.

In Node.js you don't have the document, window, and all the other objects that
are provided by the browser.

In the Browser, you don't have the API's that Node.js provides in the its
standard library, such as the filesystem functionality (fs module).

With Node.js, you can control the environment in that you can use whatever
features that your Node.js version supports.  With the Browser, you are at the
mercy of what the users choose.  If the user is running your webpage on an
outdated version of Internet Explorer, then it requires more work to make the
webpage behave as expected.


MODULES
-------------------------------------------------------------------------------
A module is an encapsulated and reusable chunk of code which has its own
context.  In Node.js, each file is treated as a separate module.

There are 3 different types of modules.

    - Local modules
        modules that we create in our application
    - Built-in modules:
        modules that Node.js ships with out of the box (standard library)
    - Third party modules
        modules written by other developers that we can use in our application


LOCAL MODULES
-------------------------------------------------------------------------------
Local modules are modules which we create in our application.  They are simply
separate javascript files.

To load a module (file) into another file, you can use the
require("./path/to/module"); function.  When the file which required the other
file is executed, the code in the required module is also executed.  However,
this does not mean that the code in the required module is available for use in
the other file.  While still executed, the code is private to that module and
can only be accessed if explicitly exported and then imported.

If the file we are requiring is a javascript or typescript file, we can skip
specifying the file extension.

CommonJS is a standard that states how a module should be structured and shared.
Node.js adopted CommonJS when it started out and it is what you will see in code
bases.  The following is an example of a CommonJS module and the require
function which is built into the Node.js Runtime.

    ----------------------------------------
    ./add.js
    -----------------------------------------
    function add(x, y) {
        return x + y;
    }

    const sum = add(5, 10);

    console.log(sum);

    ----------------------------------------
    ./demo.js
    -----------------------------------------
    require("./add");

    // console.log(sum)  This throws an error

    console.log("Hello from demo.js");

    ----------------------------------------
    terminal> node ./demo.js
    -----------------------------------------
    15
    Hello from demo.js

The require function caches modules after the first time they are loaded, which
means that subsequent require calls to the add.js file in the same or separate
modules won't re-execute the add.js file.


MODULE.EXPORTS
-------------------------------------------------------------------------------
The value of the module.exports object is what is returned from the
require function when module is required by another module.

    ----------------------------------------
    ./add.js
    -----------------------------------------
    function add(x, y) {
        return x + y;
    }

    const sum = add(5, 10);

    console.log("log statement from add.js");

    module.exports = add;

    ----------------------------------------
    ./demo.js
    -----------------------------------------
    const add = require("./add")
    // in other words: add is equal to the function 'add'
    // from ./add.js

    // console.log(sum)  This throws an error
    console.log("Hello from demo.js");
    console.log(add(5, 5));

    ----------------------------------------
    terminal> node ./demo.js
    -----------------------------------------
    log statement from add.js
    Hello from demo.js
    10

The above example uses the same name for add in both modules, but the name does
not matter of course.

This example also demonstrates the importance of knowing what exactly the
module.exports object is.  It is conventionally a js object, but could be
anything primitive or compound data type.  Knowing what the module.exports is
for a module is crucial to being able to effectively utilize said module.

The reason module.exports is typically an object is because it allows you to
bundle multiple exports into a single object.  This conveniently allows you to
make use of DESTRUCTURING ASSIGNMENT SYNTAX, which is an expression which allows
you to unpack specific properties or values from arrays or objects respectively:

    const { x, y } = { x: 10, y: () => "hello world", };
    const [a, b] = [1, 2];

    console.log(x);   // 10
    console.log(y()); // hello world
    console.log(a);   // 1
    console.log(b);   // 2


COMMONJS MODULES VS ES MODULES
-------------------------------------------------------------------------------
CommonJS Modules:
    - CommonJS is the module system originally used in Node.js.  It is designed
      for server-side JS environments.
    - Uses require() for importing modules and module.exports for exporting
    - Uses synchronous loading which means that code stops executing until the
      module has been loaded.  The behavior is generally fine for server-side
      code where modules are loaded from the local filesystem

ES Modules:
    - Also referred to as ECMAScript Modules.
    - They are the standard module system in modren JavaScript which is
      supported in both browser and recent Node.js runtimes environments.
    - Uses import and export syntax
    - Supports both synchronous and asynchronous loading.
    - When the static import statement is used at the top level of a module, the
      module is loaded synchronously.  Static imports (those without a then
      method), can only be called at the top level.

        import { moduleMember} from "./module" )

    - A dynamic import can be used anywhere in a module and are evaluated at
      runtime unlike a static import which is evaluated at compile time.
    - A dynamic import can even be used inside of a function block.
    - When the dynamic import is used it returns a Promise that resolve with the
      module which allows you to lazy load modules at any point in your code,
      not just the beginning.

        import ("./module").then(module => { /* use module */ });

    - export in Es modules are used to make local module variables (including
      classes and functions of course) available for import by other modules


MODULE SCOPE
-------------------------------------------------------------------------------
Modules (files) in javascript have private scope even when imported into other
files.  This is achieved in JS by wrapping each module into an IIFE before being
loaded.

The IIFE that wraps every module contains 5 parameters.  They are:
    exports, require, module, __filename, __dirname

__dirname:
    full path to directory of the current module
__filename:
    full path to file of the current module
require:
    function used to import a file by path
module:
    reference to the current module
exports:
    a reference to the module.exports object


MODULE CACHING
-------------------------------------------------------------------------------
NOTE: In this example we use require to import a single created instance of an
object.  Obviously, modifying by means of multiple pointers modifies the same
object, but that is NOT the point of this. If everytime the code encounters a
require statement it copies the code into the file which called require,then why
is it that there are not two separate instances of the SuperHero object in the
code example below??? That is because the JS runtime caches module imports.
Thus there is only a single instance of the SuperHero object!

    --------------------------------------------------------------
    superhero.js
    --------------------------------------------------------------
    class SuperHero {
        constructor(name) {
            this.name = name;
        }

        getName() {
            return this.name;
        }

        setName(name) {
            this.name = name;
        }
    }

    // WE EXPORT AN INSTANCE FOR THIS DEMONSTRATION, NOT THE CLASS
    // ITSELF!!!
    module.exports = new SuperHero("superman");

    --------------------------------------------------------------
    index.js
    --------------------------------------------------------------
    // FIRST REQUIRE
    const superHero = require("./superhero");

    console.log(superHero.getName());

    // SECOND REQUIRE
    const newSuperHero = require("./superhero");

    // EDIT NAME OF NEWSUPERHERO
    newSuperHero.setName("superman but edited");

    // AS EXPECTED, LOGS "SUPERMAN BUT EDITED"
    console.log(newSuperHero.getName());

    // ALSO LOGS "SUPERMAN BUT EDITED" BECAUSE OF MODULE CACHING
    console.log(superHero.getName());


IMPORT AND EXPORT PATTERNS
-------------------------------------------------------------------------------
EXAMPLE 1:

    // math.js
    function add(a, b) {
        return a + b;
    }

    module.exports = { add };

    // index.js
    const { add } = require("./math");

    console.log(add(5, 4));

-------------------------------------------------------------------------------

EXAMPLE 2:

    // math.js
    function add(a, b) {
        return a + b;
    }

    module.exports = add;

    // index.js
    const add = require("./math");

    console.log(add(5, 4));

-------------------------------------------------------------------------------
EXAMPLE 3:

    // math.js
    module.exports = function(a, b) {
        return a + b;
    }


    // OR (arrow function or regular functions syntax works)
    // module.exports = (a, b) => {
    //     return a + b;
    // }

    // index.js
    // only a single import so we can name the anonymous function what we
    // want
    const add = require("./math");

    console.log(add(5, 4));

-------------------------------------------------------------------------------
EXAMPLE 4: Publically modifying module.exports object instead of defining it

    // math.js
    module.exports.add = (a, b) => a + b;
    module.exports.subtract = (a, b) => a - b;

    // index.js
    const { add, subtract } = require("./math");
    console.log(add(5, 10), subtract(4, 3));

-------------------------------------------------------------------------------
EXAMPLE 5: Same as example 4 but shorthand (exports is a reference to
module.exports)

    // math.js
    exports.add = (a, b) => a + b;
    exports.subtract = (a, b) => a - b;

    // index.js
    const { add, subtract } = require("./math");
    console.log(add(5, 10), subtract(4, 3));


MODULE.EXPORTS VS EXPORTS
-------------------------------------------------------------------------------
If we go with the export pattern of defining the module.exports at the end of
our file, this works as expected.  However, if we try to do exports = { var };
then this will not work as expected.  Redefining exports to a new object {}
breaks the reference to module.exports and the module wrapper only returns
module.exports, not exports.  'exports' is meant to be a shorthand for
module.exports but in this case they cannot be used interchangeably.


ES MODULES
-------------------------------------------------------------------------------
REVIEW OF COMMONJS MODULES
    - Each file is treated as a module
    - Varaiables, functins, classes, etc are not accessible to other files by
      default
    - Explicitly tell the module system which parts of yourcode should be
      exported via module.exports
    - To import code into a file, use the require() function

When Node.js was created, there were no built-in module systems in JavaScript.
Node.js defaulted to CommonJS as its module system. As of ES2015, JavaScript
does have a standardized module system as part of the language itself.  That
module system is called EcmaScript Modules or ES Modules or ESM

The file extension for ES Modules is .mjs, not .js


ES MODULES IMPORT EXPORT PATTERNS
-------------------------------------------------------------------------------
EXAMPLE 1:

    // math.mjs
    function add(a, b) {
        return a + b;
    }

    export default add;

    // index.mjs
    import add from "./math.mjs";

    console.log(add(5, 5));

-------------------------------------------------------------------------------
EXAMPLE 2:

    // math.mjs
    export defaultfunction add(a, b) {
        return a + b;
    }

    export default add;

    // index.mjs
    import add from "./math.mjs";

    console.log(add(5, 5));

-------------------------------------------------------------------------------
EXAMPLE 3:

    // math.mjs
    function add(a, b) {
        return a + b;
    }

    function subtract(a, b) {
        return a - b;
    }

    export default {
        add, subtract
    }

    // index.mjs
    import math from "./math.mjs";

    console.log(math.add(5, 5));
    console.log(math.subtract(5, 5));

-------------------------------------------------------------------------------
EXAMPLE 4:

    // math.mjs
    function add(a, b) {
        return a + b;
    }

    function subtract(a, b) {
        return a - b;
    }

    export default {
        add, subtract
    }

    // index.mjs
    import math from "./math.mjs";

    // destructure math object
    const { add, subtract } = math;

    console.log(add(5, 5));
    console.log(subtract(5, 5));


-------------------------------------------------------------------------------
EXAMPLE 5:

    // math.mjs
    export function add(a, b) {
        return a + b;
    }

    export function subtract(a, b) {
        return a - b;
    }

    // index.mjs
    import * as math from "./math.mjs";

    const { add, subtract } = math;

    console.log(add(5, 5));
    console.log(subtract(5, 5));

-------------------------------------------------------------------------------
EXAMPLE 6:

    // math.mjs
    export function add(a, b) {
        return a + b;
    }

    export function subtract(a, b) {
        return a - b;
    }

    // index.mjs
    import { add, subtract } from "./math.mjs";

    console.log(add(5, 5));
    console.log(subtract(5, 5));


IMPORTING JSON
-------------------------------------------------------------------------------

    const data = require("./data.json");

When you import a json file with the require function, require will
automatically parse the contents of the json into a js object for you.  You do
NOT need to explicitly use the JSON.parse method.

You can drop the .json file extension when importing json.  However, if a
data.js file were to exist, JavaScript take precedence over importing the
data.js file over the data.json file.  So it is a good idea to use the .json
file extension even though you don't always need to.

NOTE: The above syntax does not work with the ES Modules equivalent!


--WATCH FLAG
-------------------------------------------------------------------------------

    node --watch myFile.js myOtherFile.js data.json

In my experience thus far, this is a very buggy feature especially when working
with other imported files.  Node does say this is an experimental feature.

In order for watch mode to work with imports, you can't pass in a single
argument as a file if you are importing another file into that file.  You also
need to pass in the files which are dependencies of that file.  If you don't do
this, not only will --watch mode not import the module, but it won't output
anything to the terminal.  Despite all this it still seems to run the changes
only when it feels like it.

There is also the package nodemon which does the same thing.

    sudo npm install -g nodemon


PATH MODULE
-------------------------------------------------------------------------------
The require argument can actually be written as just "path", but there are
benefits to using "node:path" instead. (1. Makes it more clear.  2. Makes the
import identifier a valid absolute URL. 3. Avoid naming conflicts for future
builtin modules or local modules)

    const path = require("node:path");

There are about 14 properties and methods associated with the path module, but
we will be focusing on just 7 here.

path.basename
    // /home/max/notes/nodejs/path/path.js
    console.log(__filename);
    // /home/max/notes/nodejs/path
    console.log(__dirname);
    // path.js
    console.log(path.basename(__filename));
    // path
    console.log(path.basename(__dirname));

path.extname
    // /home/max/notes/nodejs/path/path.js
    console.log(__filename);
    // /home/max/notes/nodejs/path
    console.log(__dirname);
    // .js
    console.log(path.extname(__filename));
    // ""
    console.log(path.extname(__dirname));

path.parse
    // combines basename, extension name, __dirname

    const obj = path.parse(__filename);
    console.log(obj);

    // {
    //   root: '/',
    //   dir: '/home/max/notes/nodejs/path',
    //   base: 'path.js',
    //   ext: '.js',
    //   name: 'path'
    // }

path.format
    // essentially does the opposite of parse

    const obj1 = path.parse(__filename);
    const format = path.format(obj1);
    console.log(format); // logs value of __filename

    const obj2 = path.parse(__dirname);
    const format = path.format(obj2);
    console.log(format); // logs value of __dirname

path.absolute
    // Is the path a full path?
    console.log(path.isAbsolute(__filename)); // true
    console.log(path.isAbsolute("./path.js")); // false


PATH.JOIN
    Joins together two paths by the platform specific separator as a delimiter
    and then normalizes the resulting path.  In other words, it works
    independent of operating system.

        console.log(path.join("folder1", "folder2", "index.html"));
        // folder1/folder2/index.html

        console.log(path.join(__dirname, "data.json"))
        // /home/max/notes/nodejs/path/data.json


    If the first argument starts with a forward slash, so will the output start
    of the path

        console.log(path.join("/folder1", "folder2", "index.html"));
        // /folder1/folder2/index.html

    path.join is good at normalizing its arguments:

        console.log(path.join("////folder1", "folder2", "index.html"));
        // still logs /folder1/folder2/index.html

        console.log(path.join("////folder1", "//folder2", "../index.html"));
        // /folder1/index.html

        console.log(path.join("////folder1", "//folder2", "./index.html"));
        // /folder1/folder2/index.html


PATH.RESOLVE
    Resolves a sequence of paths or path segments into an absolute path.
    Essentially, it creates an absolute path to whatever path is passed in as
    ...args


    /*
        * /home/max/notes/nodejs/foo/bar/index.html
    */
    console.log(path.resolve("foo", "bar", "index.html"));

    /*
         * foo starts with a / indicating it is already an absolute
         * path
         * /foo/bar/index.html
     */
    console.log(path.resolve("/foo", "bar", "index.html"));

    /*
        * bar starts with a / indicating it is already an absolute
        * path.  path.resolve will ignore the /foo and consider
        * bar as the absolute path instead because it came later
        * /bar/index.html
     */
    console.log(path.resolve("/foo", "/bar", "index.html"));

    /*
        * /index.html
     */
    console.log(path.resolve("/foo", "/bar", "/index.html"));

    /*
        * /home/max/notes/nodejs/path/index.html
     */
    console.log(path.resolve(__dirname, "index.html"));


EVENTS MODULE
-------------------------------------------------------------------------------
https://github.com/nodejs/nodejs.dev/blob/aa4239e87a5adc992fdb709c20aebb5f6da77f86/content/learn/node-js-modules/node-module-events.en.md

The events module allows us to work with events in Node.js.  An event is an
action or occurance that has happened in our application that we can respond to

    const EventEmitter = require("node:events");

    const emitter = new EventEmitter();

The events module returns a class called EventEmitter which encapsulates
functionality to emit events and respond to events.  Because it simply returns a
class, you could call it whatever you wanted, but EventEmitter is most
appropriate.

-------------------------------------------------------------------------------
EXAMPLE 1:

    const EventEmitter = require("node:events");

    const emitter = new EventEmitter();

    emitter.on("order-da-pizza", () => {
        console.log("we bakin da pizza for you brah");
        setTimeout(() => {
            console.log("should be ready for you in like 15 mins")
        }, 1000)
    })

    // When the runtime execution reaches this statement, an event
    // is broadcasted in our code
    setTimeout(() => {
        emitter.emit("order-da-pizza");
    }, 1000);

EXAMPLE 2:
    /*
        * When additional arguments are passed to the emitter.emit method
        * besides just the event name, those additional arguments are
        * automatically passed into the callback function in the emitter.on method
    */

    emitter.on("order-da-pizza", (size, topping) => {
        console.log(`Okay order recieved bro we bakin you a ${size} ${topping}`)
    })

    // When the runtime execution reaches this statement, an event
    // is broadcasted in our code
    setTimeout(() => {
        emitter.emit("order-da-pizza", "large", "pepperoni");
    }, 1000);

EXAMPLE 3:

    */
        * Multiple event listeners for the same event is okay.  They don't
        * overwrite eachother.
    */

    emitter.on("order-da-pizza", (size, topping) => {
        console.log(`Okay order recieved bro we bakin you a ${size} ${topping}`)
    })

    emitter.on("order-da-pizza", (size) => {
        console.log("You can register multiple listeners for the same event!")
        console.log("...less coding more pizza..");
        console.log("If you order a large you get a free drink");

        if (size === "large") {
            console.log("looks like you get a free drink bro");
        }
    })

    // When the runtime execution reaches this statement, an event
    // is broadcasted in our code
    setTimeout(() => {
        emitter.emit("order-da-pizza", "large", "pepperoni");
    }, 1000);


EXTENDING FROM EVENTEMITTER
-------------------------------------------------------------------------------
Classes can extend the EventEmitter classes which allows you to use all of the
functionality of the EventEmitter class within a custom class.

    const EventEmitter = require("node:events");

    class PizzaShop extends EventEmitter {
        constructor() {
            super();
            this.orderNumber = 0;
        }

        order(size, topping) {
            ++this.orderNumber;
            this.emit("order", size, topping);
        }

        displayOrderNumber() {
            console.log(this.orderNumber);
        }
    }

    const pizzaShop = new PizzaShop();

    pizzaShop.on("order", (size, topping) => {
        console.log(`Order recieved for ${size} ${topping ? topping : "no topping"}`)
    })

    pizzaShop.order("small");
    pizzaShop.displayOrderNumber();


CHARACTER SETS AND ENCODING
-------------------------------------------------------------------------------
When computers convert a character to binary format, it will first convert the
character to a number, then convert that number to its binary representation.

CHARACTER SETS are predefined lists of characters represented by numbers.
Unicode and ASCII are two examples of popular character sets.

CHARACTER ENCODING dictates how to represent a number in a character set as
binary data vefore it can be stored in a computer.  It dictates how many bits to
use to represent the number.  An example of a character encoding system is UTF-8

UTF-8 states that characters should be encoded in bytes (8 bits).  Eight 1s or
0s should be used to represent the code of any character in binary.


STREAMS
-------------------------------------------------------------------------------
A stream is a sequence of data that is being moved from one point to another
point over time.

    - a stream of data over the internet being moved from one computer to
      another
    - a stream of data being transferred form one file to another within the
      same computer

In Nodejs, we process streams of data in CHUNKS as they arrive instead of
waiting for the entire data to be available before processing.  The contents of
a stream arrive in chunks and you transfer in chunks while the remainginc
contents arrive over time.


BUFFERS EXPLANATION
-------------------------------------------------------------------------------
Imagine you are an amusement park employee at a roller coaster.  The roller
coaster has a 30 person seating capacity.

    Scenario 1:
        100 people arrive
        30 people accomodated
        70 in queue

    Scenario 2:
        1 person arrives (waiting in queue)
        ...wait until at least 10 people waiting before letting anyone board

Waiting until at least 10 people are waiting to accomodate them is a guideline
to improve efficiency.  You cannot control the pace at which people arrive, you
can only decide when the right time is to send people on the ride.

This area where people wait is analogous to the BUFFER in Node.js.  Node.js
cannot control the pace at which data arrives in the stream.  it can only decide
when is the right time to send the data for processing.

If there data already being processed, or there is too little data to be
processed, Node puts the data in BUFFER.  The buffer is an intentionally small
area that Node maintains in the runtime environment to process a stream of data.

EXAMPLE: STREAMING A VIDEO ONLINE
    If your internet connection is FAST enough, the speed of the stram will be
    fast enough to instantly fill up the buffer and send it for procesing.  This
    process will repeat until the stream is finished.

    If your connection is too SLOW, after processing the first chunk of data
    that arrived, the video player will likely display a loading spinner while
    it waits for more data to fill up the buffer.


BUFFERS CODE EXPLANATION
-------------------------------------------------------------------------------
Node.js internally handles buffers when required so actually handling them
yourself is not required in many cases.  However, here is a brief explanation of
how they work in code.


    // Buffer is a built-in class that does not need to be imported
    // utf-8 is the default decoding value so the encoding argument is optional
    const buffer = new Buffer.from("Foo", "utf-8");

    // logs an object containing an array of charcode numbers representing
    // each letter in Foo
    console.log(buffer.toJSON());

    // logs the hexadecimal representations of each letter in Foo
    // <Buffer 46 6f 6f>
    console.log(buffer);

    // returns the string representation of the binary data
    console.log(buffer.toString());

Buffers have limited memory.  They cannot grow beyond their initialized size.
However, you can overwrite a buffer.  Below is an example showing how buffers
can be written to and how they behave when overwritten.

    const buffer = new Buffer.from("FooBar", "utf-8");

    buffer.write("CODE");
    console.log(buffer.toString()); // CODEar

    buffer.write("FooBarLonger");
    console.log(buffer.toString()); // FooBar


ASYNCHRONOUS JAVASCRIPT
-------------------------------------------------------------------------------
JavaScript is a synchronous, blocking, single threaded language.

A thread is simply a process that a JavaScript program can use to run a task.
JavaScript has just one thread called the main thread for executing any code.

Web Browsers and Node.js define functions and APIs that allow us to register
functions that should not be executed synchronously and should instead be
invoked asynchronously, or in other words wait until some specific event occurs
to be ran.


FS MODULE (NOT PROMISES)
-------------------------------------------------------------------------------

    const fs = require("node:fs");

NOTE: In the two examples below, the options object passed as an argument can be
replaced by just the string "utf-8"

READFILESYNC
    const fs = require("node:fs");
    const path = require("node:path");

    // don't pass in an encoding to return just the buffer
    const buffer = fs.readFileSync(path.join(__dirname, "./file.txt"));
    console.log(buffer);

    // encodes the buffer into chars
    const contents = fs.readFileSync(path.join(__dirname, "./file.txt"), { encoding: "utf-8" });
    console.log(contents);

READFILE PROMISES
    // the callback is an example ERROR FIRST CALLBACK PATTERN
    fs.readFile(
        path.join(__dirname, "./file.txt"),
        { encoding: "utf-8" },
        (error, data) => {
            if (error) {
                console.log("woops");
            } else {
                console.log(data);
            }
        },
    );

WRITEFILESYNC
    // (path: string, contents: string)

    fs.writeFileSync(
        path.join(__dirname, "./file.txt"),
        "overwrite da whole thing",
    );


WRITEFILE

    fs.writeFile(path.join(__dirname, "./file.txt"), "da new content", (error) => {
        if (error) {
            console.log("woops");
        } else {
            console.log("file written");
        }
    });

    /*
        * add an options parameter and set the flag property to 'a' to not
        * overwrite but to append to the file
    */

    fs.writeFile(
        path.join(__dirname, "./file.txt"),
        "\nappend da content",
        { flag: "a" },
        (error) => {
            if (error) {
                console.log("woops");
            } else {
                console.log("file written");
            }

        },
    );


FS PROMISES MODULE
-------------------------------------------------------------------------------
The callback based (non promise based) versions of the fs module are preferred
when maximal performance in terms of time and memory are required.  However, the
fs promises module produces more readable code.

    const fs = require("node:fs/promises");

READFILE
    // async / await version
    async function readIt() {
        // The try catch block does work here, but what if
        // we didn't want to silently continue our program if we can't
        // read this file?  Might not always want to handle errors
        // so that we can see why our program failed
        try {
            const data = await fs.readFile(
                path.join(__dirname, "./file.txt"),
                "utf-8",
            );
            console.log(data);
        } catch (error) {
            console.error(error);
        }
    }

    readIt();

    // Promises version
    fs.readFile(path.join(__dirname, "./file.txt"), "utf-8")
        .then(console.log)

        // just like in the async function, what if we DIDN'T want to silently
        // handle this error?
        .catch(console.error);


STREAMS
-------------------------------------------------------------------------------
Stream is infact a built-in node module that inherits from the EventEmitter
class.  We rarely use streams directly, but they are used internally in other
modules.

We can create a stream using the fs module's createWriteStream and
createReadStream methods.  As mentioned, the stream class in Node extends the
EventEmitter class which is why we can create an event listener with the
readableStream.on statement in the code below.

    const fs = require("node:fs");
    const path = require("node:path");

    // Has a default buffer size of 64kb which is much greater than
    // the example file we are working with so the chunk will contain
    // the entire contents of the file
    const readableStream = fs.createReadStream(
        path.join(__dirname, "./file1.txt"),
        { encoding: "utf-8" },
    );

    const writeableStream = fs.createWriteStream(
        path.join(__dirname, "./file2.txt")
    )

    readableStream.on("data", (chunk) => {
        console.log(chunk);
        writeableStream.write(chunk);
    })

The default size of a stream is 64kb as mentioned in the comments above.  To
create a stream that has a smaller buffer size which is helpful for
demonstration purposes here, you can do so by setting the highwaterMark property
in the options object.

    const readableStream = fs.createReadStream(
        path.join(__dirname, "./file1.txt"),

        // now we deal with data in chunks of only 2 BYTES
        { encoding: "utf-8", highWaterMark: 2 },
    );

In the readableStream.on method an issue you might run into is trying to make it
concise by passing in just writeableStream.write as an argument, like this:

    readableStream.on("data", writeableStream.write);

However, do so this way will NOT work as expected because writeableStream.write
relies on the THIS context of the writeableStream object and we know that in
normal functions, the this context is inherited from the context in which the
function is called.  In this example, the write function will be invoked without
the member access operator and thus will have an undefined THIS context.  To fix
this you will need to bind it to the writeableStream object, which makes it more
verbose and kind of defeats the purpose of making the statement more concise.

    readableStream.on("data", writeableStream.write.bind(writeableStream));

Streaming data instead of just using fs.readFile and fs.writeFile are of benefit
in terms of time and memory when working with larger files that are megabytes in
size.

The http module is another example of a node module that uses streams.  Http
request is a readable stream, and http response is a writeable stream.

There are 4 types of streams:

    - READABLE STREAMS from which data can be read. (ex: reading from a file)

    - WRITEABLE STREAMS from whcih we can write data (ex: writing to a file)

    - DUPLEX STREAMS that are both readable AND writeable (ex: sockets as a
      duplex stream)

    - TRANSFORM STREAMS that can modify or transform the data as it is written
      and read (ex: file compression where you can write compressed data and
      read de-compressed data to and from a file as a transform stream)


PIPES
-------------------------------------------------------------------------------
Imagine a pipe that connects a water tank to a kitchen sink.  The tank feeds
water into the pipe which is released through the tap in the sink.  From a
Node.js pipe point of view, we a READING water from the tank and WRITING it to
the tap.

In programming paradigms, a pipe connects a readable stream and connects it to a
writeable stream.  In Node.js we can use the pipe method on a readable stream to
implement this functionality.

    const readableStream = fs.createReadStream(
        path.join(__dirname, "./file1.txt"),
        { encoding: "utf-8", highWaterMark: 2 },
    );

    const writeableStream = fs.createWriteStream(
        path.join(__dirname, "./file2.txt")
    )

    // THIS CAN BE REPLACED BY
    // readableStream.on("data", (chunk) => {
    //     writeableStream.write(chunk);
    // })

    // THIS
    readableStream.pipe(writeableStream);

Pipes return the destination stream which enables CHAINING.  However, the
destination stream must be READABLE, DUPLEX, or a TRANSFORM stream.  In the
example above, we have a WRITEABLE stream and thus we cannot make use of
chaining.

ZLIB MODULE
-------------------------------------------------------------------------------
The zlib module provides compression functionality through the zlib algorithm.
Zlib essentially allows us to create zipped files.

Zlib has a built-in transform stream.

    const zlib = require("node:zlib");

    const gzip = zlib.createGzip();

    const readableStream = fs.createReadStream(
        path.join(__dirname, "./file1.txt"),
        { encoding: "utf-8", highWaterMark: 2 },
    );

    /*
         * readableStream.pipe(gzip) returns a TRANSFORM stream
         * readable stream -> transform stream -> writeable stream
         * This writes to ./file2.txt.gz
    */
    readableStream
        .pipe(gzip)
        .pipe(fs.WriteStream(path.join(__dirname, "./file2.txt.gz")));

    const writeableStream = fs.createWriteStream(
        path.join(__dirname, "./file2.txt"),
        "utf-8",
    );
    readableStream.pipe(writeableStream);


HTTP MODULE
-------------------------------------------------------------------------------
Computers which are connected to the internet are called CLIENTS and SERVERS

CLIENTS are devices such as a computer or mobile device which are connected to
the internet via web accessing software available on those devices such as a web
browser.

SERVERS are computers that store web pages, sites, or apps.

When you type a URL into the address bar of a web browser, the CLIENT devices
REQUESTS access to a certain SERVER.  Then a copy of the webpage is downloaded
from the SERVER and sent as a RESPONSE to the client to be displayed in the web
browser.  This model is popularly called, the CLIENT SERVER MODEL.

    ##########             REQUEST               ##########
    #        # --------------------------------> #        #
    # CLIENT #                                   # SERVER #
    #        # <-------------------------------- #        #
    ##########             RESPONSE              ##########

The client server model clearly shows there is data transfer between the client
and the server.  However, in what FORMAT is that data?  What if the request from
the client cannot be understood by the server and vice versa?  This is where the
acronym HTTP comes in the picture.

HTTP stands for Hypertext Transfer Protocol, which is a protocol that defines a
FORMAT for CLIENTS and SERVERS to speak to each other.

A CLIENT sends an HTTP REQUEST and the SERVER responds with an HTTP RESPONSE.
This is a very high level abstraction of how the web works.

Node.js allows to create a web server as it has access to operating system
functionality like networking.  Given that the Node.js Runtime has an event loop
to queue asynchronous tasks, it is able to be designed to simultaneously handle
large volumes of requests.  The node server still must respect the HTTP format
and we can do this with the HTTP module.

The HTTP module allows creation of web servers that can transfer data over HTTP.

    const http = require("node:http");

    // The HTTP module extends the EventEmitter class and the callback function
    // we specify here is a request listener.  So, whenever a REQUEST reaches
    // the server, this callback is executed.
    const server = http.createServer((request, response) => {

        // 200 status code for a successful response
        response.writeHead(200, { "Content-Type": "text/plain" });
        response.end("Hello world");

    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });


EXPLANATION:
    The http.createServer method creates and returns a server object.  The
    http.createServer method accepts an event listeners callback which gets
    executed on every REQUEST.  Node automatically injects the request and
    response arguments into the callback function (because that is how the
    EventEmitter.on method works).  The request object provides information
    about the incoming request and the response object is used to send a
    response back to the client.

    The server.listen method specifies that the server we have just created
    should listen to requests on port 3000.  The server.listen method accepts a
    callback function which is executed on every request.


What actually is the request object?
    The request object contains many properties.  To see this yourself and get
    an idea of the tool you are working with, you can log it in the createServer
    methods callback function body.  It will log when the server recieves a
    request.


JSON RESPONSE
-------------------------------------------------------------------------------
This produces an error when a client (webpage) tries to access our server.

    const server = http.createServer((request, response) => {
        const foobar = {
            foo: "foo",
            bar: "bar",
        };

        response.writeHead(200, { "Content-Type": "text/plain" });
        response.end(foobar);
    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });

It produces an error because the "chunk" argument must be of type string or an
instance of Buffer or Uint8Array.  In other words, we cannot RESPOND with a
normal JavaScript object.  The stream can only understand certain types of data.
In order to fix this we need to stringify the object so that it is in JSON
format.  We can also change the format type to from "text/plain" to
"application/json"

    response.writeHead(200, { "Content-Type": "application/json" });
    response.end(JSON.stringify(foobar));

Now when the client loads the web page, the respond.end method successfully
writes the stringified object to the HTML body


HTML RESPONSE
-------------------------------------------------------------------------------
What will be outputted to the screen when the client web browser makes a request
to our server?  Will it be <h1>hello brah</h1> or simply 'hello brah' but in
larger text?

    const server = http.createServer((request, response) => {
        response.writeHead(200, { "Content-Type": "text/plain" });
        response.end("<h1>hello brah</h1>");
    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });

The above code will actually print out the literal html tags.  This is because
the "Content-Type" property is set to "text/plain".  We need to change that to
"text/html" in order to get the desired result.

    response.writeHead(200, { "Content-Type": "text/html" });

The servers send over a string like this as a response to the client, and the
client interprets this string and sees that the content type is text/html and
thus handles it appropriately.

    "HTTP/1.1\s200\sOK\nContent-Type:text/html\nContent-Length:18\n<h1>hello brah</h1>"

This above example is however not very practical.  A better way of doing this is
to use the fs module to read the contents of our index.html file before
responding with that file to the client.

    const server = http.createServer((request, response) => {
        response.writeHead(200, { "Content-Type": "text/html" });

        // readFileSync so that we wait for the file contents to be
        // read before responding!
        const html = fs.readFileSync(path.join(__dirname, "./index.html"), "utf-8");

        response.end(html);
    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });

But what if we had a very large HTML file?  It might be better to instead use a
stream instead of the fs module.

    const server = http.createServer((request, response) => {
        response.writeHead(200, { "Content-Type": "text/html" });
        fs.createReadStream(path.join(__dirname, "./index.html")).pipe(response);
        // response.end(html);
    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });


LOADING OTHER FILES LIKE JS
-------------------------------------------------------------------------------
When the response.end method writes a "text/html" file, the server sees the link
or script tags.  It then automatically creates a route for those files and will
load them accordingly.  You can therefore load a css and script tag like so:

    const PORT = process.env.PORT || 8000;

    const server = http.createServer((req, res) => {
        const content = {
            path: path.join(__dirname, "./index.html"),
            type: "text/html",
        };

        if (req.url === "/app.js") {
            content.path = path.join(__dirname, "./app.js");
            content.type = "application/javascript";
        }

        if (req.url === "/style.css") {
            content.path = path.join(__dirname, "./style.css");
            content.type = "text/css";
        }

        const file = fs.readFileSync(content.path, "utf-8");
        res.writeHead(200, { "Content-Type": content.type });
        res.end(file);
    });

    server.listen(PORT, () => {
        console.log(`Server running on port ${PORT}`);
    });


FETCHING DATA FROM THE SERVER
-------------------------------------------------------------------------------
The idea is that in the createServer method, you can route a url to RESPOND with
json data of some data from the server/database.  In the example shown, we
create a very simple mock database.  If the request.url is equal to some url we
specify, the server RESPONDS with the json data we specify.  This allows us to
pass in the endpoint to the FETCH function in our frontend js file to retrieve
the data from that endpoint.

NOTE: By convention, when creating a data endpoint, the route will start with
/api ...but this is not anything that the server checks for, its just a
convention.

MOCK DATABASE:
    const db = {
        users: [
            { name: "brodude", displayName: "Brodude" },
            { name: "max", displayName: "Max" },
            { name: "jim", displayName: "Jim" },
            { name: "joe", displayName: "Joe" },
        ],
    };

INSIDE BODY OF HTTP.CREATESERVER:
    if (request.url === "/api/users" && request.method === "GET") {
        response.writeHead(200, { "Content-Type": "text/plain" });
        response.end(JSON.stringify(db.users));
        return;
    }

APP.JS:
    const button = document.querySelector("button.clickme");
    const p = document.querySelector("p.random");

    button.addEventListener("click", fetchUserData);

    async function fetchUserData() {
        let data;
        try {
            const response = await fetch("/api/users");
            data = await response.json();
        } catch (err) {
            console.log(err);
        }

        const randNum = () => Math.floor(Math.random() * (data.length - 1));
        p.textContent = data[randNum()].name;
    }



HTML TEMPLATE
-------------------------------------------------------------------------------
In our HTML template, we will format our text in this way such that it is easy
for us to use the string.replace method to replace the formatted part with a
custom variable.

    <h1>Hello {{name}} welcome to brodude</h1>


Then in our index.js file..

    const server = http.createServer((request, response) => {
        response.writeHead(200, { "Content-Type": "text/html" });

        const name = "Foobar";
        const data = fs.readFileSync(path.join(__dirname, "./index.html"), "utf-8");
        const html = data.replace("{{name}}", name);

        response.end(html);
    });

    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });


ROUTING
-------------------------------------------------------------------------------
In the web server examples so far, we have been using https://localhost:3000 to
make server requests and view the responses.  However, so far, we also could
have entered: https://localhost:3000/about and it would have responded with the
same page.  This brings us to the concept of ROUTING.

In the http.createServer method, the request argument in the callback has a url
property which gives us the query string.

    const server = http.createServer((request, response) => {
        response.end(request.url);
    });

When we go to our client webpage and refresh, we see "/".  When we append /about
to the url in the address bar we get "/about".  Essentially, the request object
that we are given access to from the createServer method gives the server access
to the url from which the client is making the request from.  This allows us to
make a decision of how we want to handle each different url.

    const server = http.createServer((request, response) => {
        if (request.url === "/") {
            response.writeHead(200, { "Content-Type": "text/plain" });
            response.end("Home Page");
        } else if (request.url === "/about") {
            response.writeHead(200, { "Content-Type": "text/plain" });
            response.end("About Page");
        } else if (request.url === "/api") {
            response.writeHead(200, { "Content-Type": "application/json" });
            response.end(
                JSON.stringify({
                    foo: "foo",
                    bar: "bar",
                }),
            );
        } else {
            response.writeHead(404);
            response.end("Page not found");
        }
    });


    server.listen(3000, () => {
        console.log("Server is running on port 3000");
    });

The request.method method also gives us access to the HTTP methods GET POST PUT
and DELETE.  Typically a web framework will handle most of this for you.


WEB FRAMEWORK
-------------------------------------------------------------------------------
A framework abstracts much of the lower level code, which allows you to more
easily focus on the requirement than the lower level implementation. For
exxample, React, Angular, and Vue are all frontend frameworks that help you
build UIs without relying on the lower level DOM API in JavaScript.  Similarly,
there are frameworks to help build web and mobile applications without having to
explicitly rely on the HTTP module in Nodejs. Some examples of these backend
frameworks are:

    express, nest, hapi, koa, and sails

These frameworks are built on top of the HTTP module.


LIBUV
-------------------------------------------------------------------------------
libuv is a cross-platform open source library written in C/C++.  libuv is what
handles asynchronous, non-blocking operations in Node.js.  It achieves this with
the THREAD POOL and EVENT LOOP features.


THREAD POOL
-------------------------------------------------------------------------------
In Node.js when we use fs.readFileSync for example, the main thread is always
blocked until completion of the asynchronous task.

Here is an example demonstrating this using the crypto module's pbkdf2Sync method.
This method stands for Password Based Key Derivation Function 2 and it is a an
algorithm that derives a crptographic key from a password.

    const crypto = require("node:crypto");

    const start = Date.now();
    console.log(Date.now());

    crypto.pbkdf2Sync("password", "salt", 100000, 512, "sha512");
    crypto.pbkdf2Sync("password", "salt", 100000, 512, "sha512");
    crypto.pbkdf2Sync("password", "salt", 100000, 512, "sha512");
    console.log("Hash: ", Date.now() - start);

When we do not use the sync version of some of the built in asynchronous methods
in Node.js, we are really invoking Node.js to offload the process to one of the
threads in the libuv thread pool.  Once a task is completed by one of the threads
in the thread pool, libuv notifies Node.js of this completion and Node.js then
emits an event corresponding to this completion which causes any values to be
returned and any associated callbacks to be pushed onto the task queue.  The
callback then awaits its turn to be executed by the Node.js event loop.

Once that task is completed, it has Node.js
emit an Event and that will trigger Node.js to push any associated callbacks to
the task queue to await execution.

In this non-blocking example, we execute 3 calls to the crypto.pbkdf2 method at
the same relative time.  These calls get offloaded to the libuv thread pool
where they are each ran at the same time but on different threads in the thread
pool.  In other words, each of these tasks are ran in parallel.  This is what is
meant by being ran CONCURRENTLY. Then, when these tasks are done, their
associated callbacks are pushed to the task queue to await their turn to be
executed.

    const start = Date.now();
    for (let i = 0; i < 3; ++i) {
        crypto.pbkdf2("password", "salt", 100000, 512, "sha512", () => {
            console.log(Date.now() - start);
        });
    }

A perhaps obvious but important distinction to be made is that these
asynchronous operations run synchronously on their own thread which is given to
them from the thread pool.  However, since these threads are separate from the
main thread they no longer block the execution of code on the main thread and so
we call them asynchronous operations.


LIBUV THREAD POOL SIZE
-------------------------------------------------------------------------------
libuv's thread pool has only 4 threads by default.  Take the code from last
example, but this time we increase the amount of async operations to 10.

    const start = Date.now();
    for (let i = 0; i < 10; ++i) {
        crypto.pbkdf2("password", "salt", 100000, 512, "sha512", () => {
            console.log(Date.now() - start);
        });
    }

Instead of all of the tasks taking approximately 'x' time, only the first 4 take x
time.  The next 4 take 2x time, and so on...The tasks need to wait to their turn
to have an thread.

To increase the amount of threads available, we can use the following code:

    process.env.UV_THREADPOOL_SIZE = newThreadPoolSize;

However, while we can increase the amount of threads, the amount of threads we
have available depends on your hardware.  If your processor has 4 cores, then
that means we usually have 4 available threads.  If the processor has
hyper-threading then we could have 2 threads on each core.  To find the amount
of cores available you can run the lscpu command.  This computer has 6 cores and
12 threads available.  Given the amount of threads available on each computer,
the default thread pool size of 4 strikes a balance with performance.

Increasing the thread pool size to more than the amount of threads available
means that we are maximizing our threads and performance would go down as the
operating system juggles more threads over less cores.  The way the OS handles
this is to switch between threads, ensuring all processes get equal time.  The
OS tries its best to treat 16 cores as 8 for example.

Increasing thread pool size could increase performance, but that depends on the
hardware of the user.


NETWORK I/O
-------------------------------------------------------------------------------
This example uses the https (HTTP SECURE) module.  By running the code, we can
infer that the http.request method does NOT use the thread pool, since all 20 of
these operations execute in a similar amount of time

    const https = require("node:https");

    const start = Date.now();
    for (let i = 0; i < 20; ++i) {
        https
            .request("https://google.com", (res) => {
                res.on("data", () => {});
                res.on("end", () => {
                    console.log(`Request: ${i}, time: ${Date.now() - start}`);
                });
            })
            .end();
    }

The reason for this is that libuv delegates the work to the operating system
kernel, and whenever possible, it will poll the kernel and see if the request
has completed.

If there is no native async support for an operation, libuv uses the thread pool
to avoid blocking the main thread.


LIBUV AND ASYNC METHODS SUMMARY
-------------------------------------------------------------------------------
In Node.js, async operations are handled by libuv in 2 ways. These 2 ways are by
offloading to the libuv thread pool, or (in the case of the https.request
method) by using the native OS specific async mechanism.  Whenever possible,
libuv will use native async mechanisms in the OS so as to avoid blocking the
main thread.  If there is no native async support and the task is file I/O or
CPU intensive, then libuv will use the thread pool to avoid blocking the main
thread.

Although the thread pool preserves asynchronicity with respect to Node's main
thread, it can still become a bottleneck if all threads are busy.


ASYNC CODE EXECUTION AND THE LIBUV EVENT LOOP
-------------------------------------------------------------------------------
The event loop is simply an abstraction written in C++ that coordinates the
execution of synchronous and asynchronous code in Node.js.

See the 'event-loop.png' image in this folder

There are 4 different callback queues in the Node.js Event Loop, each feeding
into either the MICROTASK QUEUE.  The 4 different callback queues are all part
of libuv.  However the microtask queue is NOT part of libuv.  These 4 callback
queues part of libuv are:

    - TIMER QUEUE:
        callbacks from setTimeout, setInterval

    - I/O QUEUE:
        callbacks from I/O operations such as from the fs and https modules

    - CHECK QUEUE:
        callbacks from setImmediate.  The setImmediate function is specific to
        Node.js and is not something present in the browser runtime.

    - CLOSE QUEUE:
        close handlers

The MICROTASK QUEUE is actually two separate queues.

The first queue is called the NEXT TICK QUEUE which contains callbacks
associated with the process.nextTick function.  The process.nextTick function is
specific to Node.js.

The second queue is the PROMISE QUEUE which contains callbacks associated with
the native Promise object in JavaScript.


EVENT LOOP EXECUTION ORDER
-------------------------------------------------------------------------------
All user written synchronous JavaScript code takes priority over async code that
the runtime would like to execute.  Only once the callstack is empty does the
Event Loop become relevant to JavaScript code execution.

Callback functions are executed only when the call stack is empty.  The normal
flow of synchronous code execution will not be interrupted to run a callback
function.

The setTimeout and setInterval callbacks are given first priority.  If two async
tasks such as setTimeout and fs.readFile complete at the same time, the
setTimeout callback will be executed before the fs.readFile callback.


MICROTASK QUEUES
-------------------------------------------------------------------------------
To push a callback to the nextTick queue, you can use the process.nextTick
method. Conversely, to enqueue a callback to the Promise queue, you can use the
Promise.resolve method and immediately chain a .then method.

    Promise.resolve().then(() => {
        console.log("This is a Promise queue callback");
    });

    for (let i = 0; i < 5; ++i) {
        process.nextTick(() => {
            console.log("This is a nextTick callback");
        });
    }

    console.log(1);
    console.log(2);

    // CONSOLE:
    // 1
    // 2
    // This is a nextTick callback
    // This is a nextTick callback
    // This is a nextTick callback
    // This is a Promise queue callback

In the above code example, the .then handler callback is actually ran LAST
despite the Promise.resolve().then statement being executed first because as
stated in the above section, the nextTick queue has priority over the callback
queue.

    process.nextTick(() => {
        console.log("NT 1");
    });

    process.nextTick(() => {
        console.log("NT 2");

        process.nextTick(() => {
            console.log("Inner Tick inside of NT 2");
        });
    });

    process.nextTick(() => {
        console.log("NT 3");
    });

    Promise.resolve().then(() => console.log("PR 1"));

    Promise.resolve().then(() => {
        console.log("PR 2");
        process.nextTick(() => {
            console.log("Inner Tick inside of PR 2");
        });
    });

    Promise.resolve().then(() => console.log("PR 3"));

    // CONSOLE:
    // NT 1
    // NT 2
    // NT 3
    // Inner Tick inside of NT 2
    // PR 1
    // PR 2
    // PR 3
    // Inner Tick inside of PR 2

WHY IS THE ORDER AT THE END NOT PR1, PR2, INNER TICKET INSIDE OF PR2, PR3?
    While we say that the nextTick queue is given priority over the Promise
    queue, what we really mean is that the nextTick queue is the queue that we
    start the cycle on.  It does NOT mean that once the callstack is empty we
    ALWAYS check the nextTick queue first.  Once the callstack is empty AND the
    microtask queue is empty, then we will indeed start the cycle by checking
    the nextTick queue first. In the example above, we start by executing the
    nextTick queue cbs.  Once the nextTick queue cbs are empty, we are now in
    the Promise queue phase.  Only once the Promise queue cbs are empty will the
    nextTick queue be checked for callbacks.


PROCESS.NEXTTICK
-------------------------------------------------------------------------------
The use of process.nextTick is discouraged as it can cause the rest of the event
loop to starve.  If you endlessly call process.nextTick, the control will never
make it past the microtask queue.  You are starving the i/o queue from getting
to run its own callbacks.

According to the Node.js docs, there are two main reasons to use
process.nextTick.

    1. Allow users to handle errors, cleanup any then unneeded resources, or
    maybe try the request again before the event loop continues.

    2. To allow a callback to run after the call stack has unwound, but before
    the event loop continues.


TIMER QUEUE
-------------------------------------------------------------------------------
The timer queue is part of the MACROTASK QUEUE.

To queue a callback into the timer queue, we can use the builtin setTimeout or
setInterval functions.

    setTimeout(() => {
        console.log("foobar");
    }, 0)

NOTE: In the examples, we set the timeout to 0, so the setTimeout callbacks are
immediately queue'd into the Timer queue.  While it may seem obvious, its easy
to forget that the setTimeout cbs are queue'd only once the timer has finished.
This is important to keep in mind as adding an actual timer on the setTimeout
function of course adds another level.

NOTE: Technically, the Timer queue is NOT a queue data structure, but a MIN HEAP
data structure.  Thinking about it as a queue makes the process simpler.

Once the call stack is empty, the event loop always checks the MICROTASK queue
first and will only check the MACROTASK queue once the MICROTASK queue is empty.
To get a better understanding, trace the execution of this code:

    // TIMER (MACROTASK)
    setTimeout(() => {
        console.log("ST 1");
    }, 0);
    setTimeout(() => {
        console.log("ST 2");
        process.nextTick(() => {
            console.log("Inner Tick inside of ST 2");
        });
        Promise.resolve().then(() => {
            console.log("Inner PR 4 inside ST 2");
        });
    }, 0);
    setTimeout(() => {
        console.log("ST 3");
    }, 0);

    // NEXT TICK (MICROTASK)
    process.nextTick(() => {
        console.log("NT 1");
    });
    process.nextTick(() => {
        console.log("NT 2");

        process.nextTick(() => {
            console.log("Inner Tick inside of NT 2");
        });
    });
    process.nextTick(() => {
        console.log("NT 3");
    });

    // PROMISE (MICROTASK)
    Promise.resolve().then(() => console.log("PR 1"));
    Promise.resolve().then(() => {
        console.log("PR 2");
        process.nextTick(() => {
            console.log("Inner Tick inside of PR 2");
        });
    });
    Promise.resolve().then(() => console.log("PR 3"));

    // CONSOLE:
    // NT 1
    // NT 2
    // NT 3
    // Inner Tick inside of NT 2
    // PR 1
    // PR 2
    // PR 3
    // Inner Tick inside of PR 2
    // ST 1
    // ST 2
    // Inner Tick inside of ST 2
    // Inner PR 4 inside ST 2
    // ST 3


IO QUEUE
-------------------------------------------------------------------------------
The 2nd phase in the MACROTASK queue.

Most of the async methods from the builtin Node.js modules queue the callback
argument into the I/O queue.  For the examples in this section, we will be using
the fs.readFile method.

    const fs = require("node:fs");

    // No options argument because we don't care about using
    // the data for this demonstration.  We only care
    // about the callback order.
    fs.readFile(__filename, () => {
        console.log("READ 1");
    });

    fs.readFile(__filename, () => {
        console.log("READ 2");
        setTimeout(() => {
            console.log("Inner ST inside of READ 2");
        }, 0);
    });

    setTimeout(() => {
        console.log("ST 1");
    }, 0);

    process.nextTick(() => console.log("NT 1"));
    Promise.resolve().then(() => console.log("PR 1"));

    // CONSOLE:
    // NT 1
    // PR 1
    // ST 1
    // READ 1
    // READ 2
    // Inner ST inside of READ 2


NOTE: When running setTimeout with a 0ms delay along with an I/O async method,
the order of execution can never be gauranteed.  The below code always logs ST1
first on this OS, but could log READ 1 first sometimes and ST 1 first sometimes
on a different system.

    const fs = require("node:fs");
    setTimeout(() => {
        console.log("ST 1");
    }, 0);
    fs.readFile(__filename, () => {
        console.log("READ 1");
    });

Why does this anomoly occur?  This is due to how a minimal delay is set for
timers.  In the DOM timer and Node.js timer, a delay set to 0ms is overridden to
be 1ms.  A 1ms difference is irrelevant to most use cases, but is enough to
throw off the order of execution in this specific example.  (On my system, a 3ms
delay is just enough to create variance in the results)

    const fs = require("node:fs");

    fs.readFile(__filename, () => {
        console.log("READ 1");
    });
    process.nextTick(() => {
        console.log("NT 1");
    });
    Promise.resolve().then(() => {
        console.log("PR 1");
    });
    setTimeout(() => {
        console.log("ST 1");
    }, 0);

    // Ensure that setTimout timer has elapsed before control
    // enters the Timer queue
    for (let i = 0; i < 20000000; ++i) {}

    // CONSOLE:
    // NT 1
    // PR 1
    // ST 1
    // READ 1


I/O POLLING (and some of the CHECK QUEUE)
-------------------------------------------------------------------------------
I/O polling happens in between the I/O queue and the CHECK queue.

To queue a callback into the CHECK queue, we can use the builtin function
setImmediate.

When control  reaches the I/O queue, if there are any callbacks present it will
be execute them as expected.  However, in order for callbacks to be pushed to
the I/O queue, the control must first go to the I/O polling part of the loop
where it asks the pending I/O operations if they have completed.  If yes, then
the associated callback is pushed to the I/O queue.  However, control is passed
the I/O queue and thus the callback is not executed.  The control then proceeds
to the CHECK queue and executes any callbacks there.  Once control makes its way
back to the I/O queue, the callback is finally executed.

    const fs = require("node:fs");

    fs.readFile(__filename, () => {
        console.log("READ 1");
    });
    process.nextTick(() => {
        console.log("NT 1");
    });
    Promise.resolve().then(() => {
        console.log("PR 1");
    });
    setTimeout(() => {
        console.log("ST 1");
    }, 0);
    setImmediate(() => console.log("IMMED 1"));

    for (let i = 0; i < 20000000; ++i) {}

    // CONSOLE:
    // NT 1
    // PR 1
    // ST 1
    // IMMED 1
    // READ 1

IO events are POLLED and callback functions are added to the IO queue only after
the IO task has been both polled and has completed.


CHECK QUEUE
-------------------------------------------------------------------------------
To queue a callback into the CHECK queue, we can use the builtin function
setImmediate.

Check queue callbacks are executed AFTER:
    - MICROTASK queue callbacks
    - TIMER queue callbacks
    - I/O queue callbacks

However, as explained in the section above, its important to note that for I/O
queue callbacks to be queue'd, they must first pass the I/O polling part of the
event loop.

    fs.readFile(__filename, () => {
        console.log("READ 1");
        fs.readFile(__filename, () => {
            console.log("READ inside READ 1");
        });
        setImmediate(() => {
            console.log("IMMED inside READ 1");
        });
        process.nextTick(() => {
            console.log("NT inside READ 1");
        });
        Promise.resolve().then(() => {
            console.log("PR inside READ 1");
        });
    });
    process.nextTick(() => {
        console.log("NT 1");
    });
    Promise.resolve().then(() => {
        console.log("PR 1");
    });
    setTimeout(() => {
        console.log("ST 1");
    }, 0);
    setImmediate(() => {
        console.log("IMMED 1");
    });

    for (let i = 0; i < 20000000; ++i) {}

    // CONSOLE:
    // NT 1
    // PR 1
    // ST 1
    // IMMED 1
    // READ 1
    // NT inside READ 1
    // PR inside READ 1
    // IMMED inside READ 1
    // READ inside READ 1

-------------------------------------------------------------------------------

    setImmediate(() => console.log("SI 1"));
    setImmediate(() => {
        console.log("SI 2");
        process.nextTick(() => console.log("NT in SI 2"));
        Promise.resolve().then(() => console.log("PR in SI 2"));
    });
    setImmediate(() => console.log("SI 3"));

    // CONSOLE:
    // SI 1
    // SI 2
    // NT in SI 2
    // PR in SI 2
    // SI 3


CLOSE QUEUE
-------------------------------------------------------------------------------
To queue a callback into the CLOSE queue, you can add an event listener to a
"close" event.

NOTE: In the example below, we close the readable stream, THEN in the next line
we create the event listener for the close event.  This might seem wierd as
usually you would have defined the close event listener before closing, but
presumably, when we close the readableStream object we output an event and that
event is present for some amount of time or maybe for the lifetime of the
readableStream object before garbage collection.  The close callback is then
stored as a variable in the 'on' property of the readableStream object and
pushed to the task queue only when the close event happens on the readableStream
object.  By creating closing the stream before setting the on listener, we
can push the callback to the task queue immediately and more easily be able to
see the order of execution in regards to other callbacks.

    const fs = require("node:fs");

    const readableStream = fs.createReadStream(__filename);

    readableStream.close();
    readableStream.on("close", () => {
        console.log("READABLE CLOSE");
    });

    setImmediate(() => console.log("SI 1"));
    setTimeout(() => console.log("ST 1"), 0);
    Promise.resolve().then(() => console.log("PR 1"));
    process.nextTick(() => console.log("NT 1"));

    // CONSOLE:
    // NT 1
    // PR 1
    // ST 1
    // SI 1
    // READABLE CLOSE


EVENT LOOP / TASK QUEUE SUMMARY
-------------------------------------------------------------------------------
If you refer to the diagram, the outer loop is the timer, io, check, and close
queues along with the io poll.  The inner loop is the next tick and Promise
queues.  If it helps to think of the outer loop as the macrotask queue and the
inner loop as the microtask queues, that is okay I think, but I don't believe
this is official Node.js terminology.  It would be more accurate to
refer to the parts of the system by their names and understand that the nextTick
and Promise queues are always checked first after completing execution of the
call stack.  The inner and outer loop both keep track of their positions between
call stack completions, but the inner loop starts at the next tick queue
and the outer loop starts at the timer queue.


NPM
-------------------------------------------------------------------------------
Npm stands for Node Package Manager.  It is software package manager and the
world's largest software library.

A package.json file is npm configuration file.  It is a json file that typically
lives in the root directory of your package and holds various metadata relevant
to the package.   It is used to configure and describe how to interact with an
run your package.  It is primarily used by the npm CLI.

    {
        "name": "greet-brodude",
        "version": "1.0.0",
        "description": "brodude greeting package",
        "keywords": [ "brodude", "greet" ],
        "main": "index.js",
    }

This is a sample package.json file, but typically you would use the npm CLI to
create a package.json file with the 'npm init' command.  npm init --yes | -y to
default initialize package.json file.  In order to actually install packages
from npm locally, you need to initialize a package.json file.

Imagine you git clone a repository but the repository has the node_modules
folder ignored.  When you try to run the entry point, it obviously won't work
because you didn't clone the node_modules.  The package.json file makes the
solution to this easy.  Simply enter the command "npm install" and npm will
install all of the dependencies listed in the package.json file.  When you
install a package, npm automatically updates your package.json file as well.
All of the installing and uninstalling of packages is automated through npm.


VERSIONING PACKAGES AND SEMANTIC VERSIONING
-------------------------------------------------------------------------------
To install a specific version of a package:

    npm install package-name@version-number

Example: npm install lodash@1.1.1

Semantic Versioning (SemVer) is one of the most widely adopted versioning
systems.

    version x.y.z

X stands for a major version
Y stands for a minor version
Z stands for a patch version

When you fix a bug and the code still stays backwards compatible, you increment
the PATCH version. (1.1.1 ---> 1.1.2)

When you add a new functionality, but the code still stays backwards compatible,
you increment the MINOR version.  In addition, you also reset the patch version
to zero. (1.1.1 ----> 1.2.0)

When you make changes and the code is no longer backwards compatible, you
increment the MAJOR version and reset the minor and patch versions to zero.
(1.1.1 ---> 2.0.0)

Semantic versioning always starts with 0.1.0.  0.Y.Z is used for INITIAL
DEVELOPMENT.  When the code is PRODUCTION READY, you increment the version to
1.0.0.  Even the simplest of changes has to be done with an increase in the
version number.


NPM SCRIPTS
-------------------------------------------------------------------------------
Common use cases for npm scripts include building your project, starting a
development server, compiling CSS, linting, minifying, etc...

    npm run <script-name>

Inside the package.json file...

    "scripts": {
        "start": "node index.js"
    },


MINI PROJECT: BUILDING A CLI TOOL AND PUBLISHING
-------------------------------------------------------------------------------
In order to create a cli application, we need to add a shebang.

    #!/usr/bin/env node

    console.log("hello world");

NOTE: The shebang ALSO allows us to simply execute a js file without needing to
prefix with the 'node' command.  For the example above we just need to make the
file executable (chmod +x) and then we can run it with ./script.js just like an
executable .sh file.  For a simple example like the above you could add the
--eval flag to do the same thing!

    node --eval "console.log('hello world')"

-e is the short version of --eval btw...

We also need to create a new property in our package.json file called "bin".
The bin field allows us to treat our package as an executable file and allows
our package to be installed to the path variable.  The value of the bin property
is another object. The key is the name of the command we want to use to
run our CLI or application.  The value is the ENTRY point to the cli:

    "bin": {
        "brodude": "index.js"
    }

Here is the finished package.json:

    {
      "name": "brodude-cli",
      "version": "1.0.0",
      "main": "index.js",
      "scripts": {
        "test": "echo \"Error: no test specified\" && exit 1"
      },
      "keywords": [],
      "author": "",
      "license": "ISC",
      "description": "",
      "bin": {
        "brodude": "index.js"
      }
    }

Now we just need to global install our app.  We can do this without publishing
to the npm registry even.  To do this without publishing to the registry, simply
run "sudo npm install -g" in the root directory of the project.  To uninstall
it, run "sudo npm uninstall -g" in the root directory of the project.

Here is a modified index.js that now fetches data from the pokeapi.  Right now
we are simply calling the async function that fetches data with a predefined
value, however, in the next step we will pass in arguments to make our cli more
practical.

    #!/usr/bin/env node

    async function printFiveMoves(pokemanName) {
      const response = await fetch(
        `https://pokeapi.co/api/v2/pokemon/${pokemanName}`,
      );
      const pokeman = await response.json();
      const moves = pokeman.moves.map((move) => move.move.name);
      console.log(moves.slice(0, 5));
    }

    printFiveMoves("charmander");

The CLI is called by entering the command "brodude".

NOTE: We do not need to keep running "sudo npm install -g" to update our CLI
when we make changes.  Simply making changes to our CLI app is enough.  When we
global install a local package like we did, node simply creates a symbolic link
from /usr/bin/<bin-name> to our package.


PROCESS.ARGV
-------------------------------------------------------------------------------
We just added a log statement to print out process.argv

    #!/usr/bin/env node

    console.log(process.argv);

Now we run our script "brodude charmander".  The process.argv actually prints
out an array of arguments passed to the command.  However, the first two
arguments are always implicitly passed and the third argument, if it exists, is
what we pass as an argument when entering the command ("charmander" in this
case).

~/void/customcli > brodude charmander
    [ '/usr/bin/node', '/usr/bin/brodude', 'charmander' ]
    [
      'mega-punch',
      'fire-punch',
      'thunder-punch',
      'scratch',
      'swords-dance'
    ]

The 0th index of process.argv is always the PATH TO THE INTERPRETER
The 1st index of process.argv is always the PATH TO THE CLI command

Now we can pass in process.argv[2] to give users of our CLI the ability to
choose whatever they want to pass into our printFiveMoves function.  This might
work for a simple application, but for something more complex, a package that
handles this for us is advisable (yargs for example).

    if (process.argv[2]) {
      printFiveMoves(process.argv[2]);
    } else {
      console.log("bro add a pokeman!");
    }

With yargs (npm install yargs), we can do the same thing.

    #!/usr/bin/env node

    // import yargs
    const yargs = require("yargs");

    // create a argv object using the yargs function
    const { argv } = yargs(process.argv);

    async function printFiveMoves(pokemanName) {
      const response = await fetch(
        `https://pokeapi.co/api/v2/pokemon/${pokemanName}`,
      );
      const pokeman = await response.json();
      const moves = pokeman.moves.map((move) => move.move.name);
      console.log(moves.slice(0, 5));
    }

    // run our function if we supply an argument.  Otherwise tell the user how
    // to use our CLI
    if (argv.pokeman) {
      printFiveMoves(argv.pokeman);
    } else {
      console.log("bro add a pokeman! ex: brodude --pokeman charmander");
    }


USER INPUT FOR CUSTOM SCRIPTS
-------------------------------------------------------------------------------
In the example above we used yargs to allow our user to supply a custom argument
with the --pokeman flag!  But what if we wanted to create prompt instead to
allow our users to choose a pokeman instead?  The INQUIRER package is good for
this.

    npm install inquirer

NOTE: version 9 of inquirer only works with ES Modules, so for this
demonstration, we will be using version 8.2.5 (npm install inquirer@8.2.5)

It might seem incredibly obvious or not so obvious, but user input is of course
an async operation.  Therefore, inquirer makes use of Promises.  It comes with a
method: inquirer.createPromptModule which returns a function to create a prompt.
When you call that function, it creates a prompt and when the user enters input,
the Promise resolves and you can handle the result of the Promise.  Using
async/await would definitely be cleaner than the example implementation below.

    #!/usr/bin/env node

    const inquirer = require("inquirer");

    async function printFiveMoves(pokemanName) {
      const response = await fetch(
        `https://pokeapi.co/api/v2/pokemon/${pokemanName}`,
      );
      const pokeman = await response.json();
      const moves = pokeman.moves.map((move) => move.move.name);
      console.log(moves.slice(0, 5));
    }

    // returns a function function that can be used to create
    // prompts
    const prompt = inquirer.createPromptModule();

    // The prompt function returns a Promise, so we can use the .then method
    // to handle the result of the user input
    prompt([
      {
        type: "input",
        name: "pokeman",
        message: "Enter da pokeman name to view its first 5 moves:",
      },
    ])
      .then((answers) => {
        return answers.pokeman;
      })
      .then((pokeman) => {
        return printFiveMoves(pokeman);
      })
      .then(() => {
        prompt([
          {
            type: "input",
            name: "brodude",
            message: "Enter another name brah: ",
          },
        ]).then((answers) => {
          const brodude = answers.brodude;
          console.log("nah I don't feel like fetching any more information");
        });
      });


CLUSTER MODULE
-------------------------------------------------------------------------------
In this example we create a simple server that will respond quickly on the
homepage, and then slow on the /slow-page route due to the intentional for loop
before running the res.writeHead and res.end methods.

    const http = require("node:http");

    const server = http.createServer((req, res) => {
      if (req.url === "/") {
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Home Page");
      } else if (req.url === "/slow-page") {
        // perform 5 billion iterations before doing anything
        // to simulate a slow page
        for (let i = 0; i < 5000000000; ++i) {}

        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Slow Page");
      }
    });

    server.listen(8000, () => console.log("Server running on port 8000"));

NOTE: Use the Network section on on DevTools to see the response time when
refreshing the page (around 16 seconds due to the for loop here)

Clearly this impacts the /slow-page route, but it actually effects our home page
as well!  Sort of, if the user goes to the home page it initially loads very
quickly as expected.  However, if the user then travels to /slow-page and then
back to home page, the homepage will actually take significantly longer to load
now.  This is because Node.js is single threaded, and slow-page is blocking home
page from loading until it completes its loop.

One way to fix this is to use the CLUSTER MODULE.

cluster.js
--------------------------------------------------
    const cluster = require("node:cluster");

    module.exports.handleWorkers = (workerCb) => {
      if (cluster.isMaster) {
        console.log(`Master process ${process.pid} is running`);
        cluster.fork();
        cluster.fork();
      } else {
        console.log(`Worker ${process.pid} started`);
        workerCb();
      }
    };

index.js
--------------------------------------------------
    const http = require("node:http");
    const { handleWorkers } = require("./cluster");

    function createServer() {
      const server = http.createServer((req, res) => {
        if (req.url === "/") {
          res.writeHead(200, { "Content-Type": "text/plain" });
          res.end("Home Page");
        } else if (req.url === "/slow-page") {
          for (let i = 0; i < 5000000000; ++i) {}
          res.writeHead(200, { "Content-Type": "text/plain" });
          res.end("Slow Page");
        }
      });

      server.listen(8000, () => console.log("Server running on port 8000"));
    }

    handleWorkers(createServer);

Now when we switch from /slow-page back to home page, the response time is
immediate.

NOTE: If we were to simultaneously load /slow-page and homepage in different
tabs at relatively the same time, the same effect would occur without this
solution.

This is a great solution right?  Depends...its not infinitely scalable.  As a
general rule, we should only create as many workers as there are CPU cores on
the host machine.  If you create more workers than there are cores, then it can
cause an overhead as the system will have to schedule all the created workers
with a fewer number of cores.  (This is explained in more detail in the section
on the libuv thread pool).

To see how many cores your host machine has:

    const os = require("node:os");
    console.log(os.cpus().length);

Another way of handling this problem is with the pm2 package.

    sudo npm install -g pm2

Now we can use pm2 to handle our clusters for us.  I have changed index.js to be
a simple server again:

    const http = require("node:http");

    const server = http.createServer((req, res) => {
      if (req.url === "/") {
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Home Page");
      } else if (req.url === "/slow-page") {
        for (let i = 0; i < 5000000000; ++i) {}
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Slow Page");
      }
    });

    server.listen(8000, () => console.log("Server running on port 8000"));

The following command will optimally designate workers for us:

    pm2 start index.js -i 0

To stop the server:

    pm2 stop index.js


WORKER THREADS MODULE
-------------------------------------------------------------------------------
The worker_threads module enables the use of threads the execute JavaScript in
parallel..  Code executed ina  worker thread runs in a separate child process,
which prevents it from blocking your main application.

How is this different from the cluster module?:
    - The CLUSTER module can be used to run multiple instances ofNode.js that
      can distribute workloads

    - The WORKER_THREADS module allows running mutliple applicationt hreads
      within a single Node.js instance.

When process isolation is not needed, (no separate instances of V8 are needed,
which includes event loop and memory), you should use worker_threads

Good use cases for the worker_threads module would be resizing images or videos.

EXAMPLE:

index.js
-------------------------------------------------------------------------------
    const http = require("node:http");

    // get the Worker constructor from the module
    const { Worker } = require("node:worker_threads");

    const server = http.createServer((req, res) => {
      if (req.url === "/") {
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Home Page");
      } else if (req.url === "/slow-page") {
        // create new worker
        const worker = new Worker("./worker-thread.js");

        // when the worker has finished its task, execute a callback
        worker.on("message", (j) => {
          res.writeHead(200, { "Content-Type": "text/plain" });
          res.end(`Slow Page: ${j}`);
        });
      }
    });

    server.listen(8000, () => console.log("Server running on port 8000"));

worker-thread.js
-------------------------------------------------------------------------------
    // import the parentPort object which we can use
    // to create the "message" event that the Worker object uses
    const { parentPort } = require("node:worker_threads");

    let j = 0;
    for (let i = 0; i < 1000000000; ++i) {
      ++j;
    }

    parentPort.postMessage(j);


DEPLOYING A NODE APPLICATION
-------------------------------------------------------------------------------
There are many services to deploy a Node.js application.  To list a few there
is:
    Heroku (not free)
    Fly.io
    Render

Just follow this video:

https://www.youtube.com/watch?v=yln_CffenYw&list=PLC3y8-rFHvwh8shCMHFA5kWxD9PaPwxaY&index=63


END OF CODEVOLUTION NODE.JS TUTORIAL.  WHERE TO GO FROM HERE:
-------------------------------------------------------------------------------
Express.js
    - Express.js is a web framework built on top of Node.js

Jest or Vitest
    - Write unit tests for an application


HOW TO READ ENV VARIABLES FROM NODE.JS
-------------------------------------------------------------------------------
❯ USER_ID=123456 USER_KEY=foobar node script.js

And then inside of script.js we can access those ENV variables we set with
process.env

    console.log(process.env.USER_ID); // logs '123456'
    console.log(process.env.USER_KEY); // logs 'foobar'

Node.js 20 introduced EXPERIMENTAL support for .env files.  Now you can use the
--env-file flag to specify and environemnt file when running your Node.js
application.  For example:

.my-env-vars.env
---------------------
PORT=3000

script.js
---------------------
console.log(process.env.PORT || "no port env variable provided");

Command-line: node --env-file=.my-env-vars.env script.js
LOGS: 3000

If we have multiple --env-file flags, the last one overwrites preexisting
variables defined in previous files.  Suppose in our .dev.env we had the PORT
variable set to 5000 instead of 3000...

❯ node --env-file=.my-env-vars.env --env-file=.dev.env script.js

This command now logs 5000 instead

NOTE: If you have jsut a .env file in your root directory and you run 'node
index.js', then Node will NOT automatically pull env variables from your .env
file.  In order to get that functionality you would need to use the 'dotenv'
package for that:

1. npm install dotenv
2. Import the 'dotenv' module and then call its config method at the top level
of your index script

(^^^ haven't tested this yet....)



PERFORMING A GET REQUEST / AXIOS MODULE / HTTPS MODULE
-------------------------------------------------------------------------------
https://github.com/nodejs/nodejs.dev/blob/aa4239e87a5adc992fdb709c20aebb5f6da77f86/content/learn/node-js-web-server/node-make-http-requests.en.md
-------------------------------------------------------------------------------
NOTE: This section uses the HTTPS module, NOT the HTTP module!

A simple way of performing an HTTP GET request is to use the axios module.

NOTE: The axios module is not a built-in Node.js module.  You need to install it
locally! (npm install axios)

    const axios = require("axios");

    axios
        .get("https://google.com")
        .then((res) => {
            console.log(res.status);
            // console.log(res); // logs a huge object
        })
        .catch(console.error);


But Node.js also allows you to use standard Node.js modules to do the same
thing, albeit a bit more verbose than the axios example.  This does more or less
the same thing as the axios example:

    const https = require("node:https");

    const options = {
        hostname: "google.com",
        port: 443,
        path: "/",
        method: "GET",
    };

    const request = https.request(options, (response) => {
        console.log(`statusCode: ${response.statusCode}`);

        response.on("data", (d) => {
            process.stdout.write(d);
        });
    });

    request.on("error", console.error);
    request.end();

NOTE: The statement: 'process.stdout.write(d)' prints a BUFFER if we console.log
it!  It appears that process.stdout.write encodes the buffer for us.

Just like making an HTTP GET request, you can use the Axios libraryto perform a
POST request as well.

    const axios = require("axios");

    axios
        .post("https://google.com/todos", {
            todo: "Buy the milk!",
        })
        .then((response) => {
            console.log(`statusCode: ${response.status}`);
        })
        .catch((e) => console.error(e.message));

The catch block will of course run in this dummy example, but its a proof of
concept.

And heres a similar example using the Node.js HTTPS module (which will also fail
of course):

    const https = require("https");

    // Can't send over plain JavaScript object!
    // We need to stringify it first!
    const data = JSON.stringify({
        todo: "Buy the milk!",
    });

    const options = {
        hostname: "google.com",
        port: 443,
        path: "./todos",
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "Content-Length": data.length,
        },
    };

    const request = https.request(options, (response) => {
        console.log(`statusCode: ${response.statusCode}`);

        response.on("data", (d) => {
            process.stdout.write(d);
        });
    });

    request.on("error", (e) => {
        console.error(e.message)
    });
    request.write(data);
    request.end();

PUT and DELETE request follow the same POST request format, you just need to
changes the options.method accordingly.


FS MODULE METHODS
-------------------------------------------------------------------------------
The fs module contains more method than just writeFile and readFile.  This link
provides a list of the available options:

https://github.com/nodejs/nodejs.dev/blob/aa4239e87a5adc992fdb709c20aebb5f6da77f86/content/learn/node-js-modules/node-module-fs.en.md

Here is the Node.js guide on writing files, which is more explanatory the raw fs
docs which are more of a reference:

https://nodejs.org/en/learn/manipulating-files/writing-files-with-nodejs

The above link goes into fs.appendFile which appends content to a file

Same as the above but for reading files:

https://nodejs.org/en/learn/manipulating-files/reading-files-with-nodejs


URL CLASS
-------------------------------------------------------------------------------
https://nodejs.org/api/url.html#url_the_whatwg_url_api

The URL constructor creates a new URL object from the url string argument.

    const myURL = new URL("https://example.org/foo/bar?baz");
    console.log(myURL.origin); // logs: https://example.org


UPLOADING FILES WITH FORMIDABLE MODULE
-------------------------------------------------------------------------------
https://www.w3schools.com/nodejs/nodejs_uploadfiles.asp

This tutorial starts out okay...but the saving the file section is confusing.


SENDING EMAILS WITH THE NODEMAILER MODULE
-------------------------------------------------------------------------------
https://nodemailer.com/usage/using-gmail/

This probably won't work with gmail due to gmail blocking it because it looks
suspicious.

    const nodemailer = require("nodemailer");

    const transporter = nodemailer.createTransport({
        service: "gmail",
        auth: {
            user: "example@gmail.com",
            pass: "password",
        },
    });

    const mailOptions = {
        from: "example@gmail.com",
        to: "otherexample@gmail.com",
        subject: "foobar",
        text: "foobar email body",
    };

    transporter.sendMail(mailOptions, (err, info) => {
        if (err) {
            console.log(err.message);
        } else {
            console.log(info.response);
        }
    });









